{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74386924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#statistical analysis \n",
    "import statsmodels.api as sm\n",
    "\n",
    "#ML models \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, IsolationForest\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.model_selection import train_test_split  # This line was missing\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d2e99",
   "metadata": {},
   "source": [
    "## Reading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6a23fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>q_id</th>\n",
       "      <th>d_id</th>\n",
       "      <th>relevancy_score_1</th>\n",
       "      <th>relevancy_score_2</th>\n",
       "      <th>category_id</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>price</th>\n",
       "      <th>target_score</th>\n",
       "      <th>relevancy_score_1_rank</th>\n",
       "      <th>relevancy_score_2_rank</th>\n",
       "      <th>price_rank</th>\n",
       "      <th>discount_rank</th>\n",
       "      <th>search_view_mean</th>\n",
       "      <th>search_view_std</th>\n",
       "      <th>search_click_std</th>\n",
       "      <th>acceptance_ratio_mean</th>\n",
       "      <th>search_view_rank</th>\n",
       "      <th>search_sales_rank</th>\n",
       "      <th>acceptance_ratio_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>351626</td>\n",
       "      <td>1029.9591</td>\n",
       "      <td>0.692121</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2555000.0</td>\n",
       "      <td>0.621563</td>\n",
       "      <td>20.5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1851.6</td>\n",
       "      <td>251.503567</td>\n",
       "      <td>26.601796</td>\n",
       "      <td>0.047858</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>392222</td>\n",
       "      <td>1029.9591</td>\n",
       "      <td>0.273830</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2450000.0</td>\n",
       "      <td>0.589059</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1660.3</td>\n",
       "      <td>209.068330</td>\n",
       "      <td>22.299975</td>\n",
       "      <td>0.060965</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>475019</td>\n",
       "      <td>1029.9591</td>\n",
       "      <td>0.316562</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.584239</td>\n",
       "      <td>20.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>480445</td>\n",
       "      <td>1029.9591</td>\n",
       "      <td>0.318788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1317500.0</td>\n",
       "      <td>0.579027</td>\n",
       "      <td>20.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>728.6</td>\n",
       "      <td>170.300519</td>\n",
       "      <td>9.989439</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>286374</td>\n",
       "      <td>1029.9591</td>\n",
       "      <td>0.397037</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5380000.0</td>\n",
       "      <td>0.577228</td>\n",
       "      <td>20.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1079.8</td>\n",
       "      <td>212.732383</td>\n",
       "      <td>10.622827</td>\n",
       "      <td>0.044706</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  q_id    d_id  relevancy_score_1  relevancy_score_2  \\\n",
       "0           0     0  351626          1029.9591           0.692121   \n",
       "1           1     0  392222          1029.9591           0.273830   \n",
       "2           2     0  475019          1029.9591           0.316562   \n",
       "3           3     0  480445          1029.9591           0.318788   \n",
       "4           4     0  286374          1029.9591           0.397037   \n",
       "\n",
       "   category_id  brand_id      price  target_score  relevancy_score_1_rank  \\\n",
       "0          1.0       1.0  2555000.0      0.621563                    20.5   \n",
       "1          1.0       1.0  2450000.0      0.589059                    20.5   \n",
       "2          1.0       1.0        NaN      0.584239                    20.5   \n",
       "3          1.0       1.0  1317500.0      0.579027                    20.5   \n",
       "4          1.0       1.0  5380000.0      0.577228                    20.5   \n",
       "\n",
       "   relevancy_score_2_rank  price_rank  discount_rank  search_view_mean  \\\n",
       "0                    23.0         7.0           20.0            1851.6   \n",
       "1                     2.0         5.5           20.0            1660.3   \n",
       "2                     7.0         NaN           20.0               NaN   \n",
       "3                    10.0         1.0           40.0             728.6   \n",
       "4                    14.0         8.0           20.0            1079.8   \n",
       "\n",
       "   search_view_std  search_click_std  acceptance_ratio_mean  search_view_rank  \\\n",
       "0       251.503567         26.601796               0.047858              10.0   \n",
       "1       209.068330         22.299975               0.060965               9.0   \n",
       "2              NaN               NaN                    NaN               NaN   \n",
       "3       170.300519          9.989439               0.036558               7.0   \n",
       "4       212.732383         10.622827               0.044706               8.0   \n",
       "\n",
       "   search_sales_rank  acceptance_ratio_rank  \n",
       "0               10.0                    9.0  \n",
       "1                9.0                   10.0  \n",
       "2                NaN                    NaN  \n",
       "3                7.0                    7.0  \n",
       "4                3.5                    8.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4d0dc1",
   "metadata": {},
   "source": [
    "## Train-Test Split:\n",
    "In this step, we partitioned the data into training and testing sets using an 80-20 split. Crucially, we treated `q_id` as our grouping variable to ensure that all instances of a given `q_id` are exclusively in either the training or testing set, but not both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53cf08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to split train and test without conflict with `q_id`s \n",
    "gss = GroupShuffleSplit(test_size=0.20, n_splits=1, random_state = 0).split(df, groups=df['q_id'])\n",
    "\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "\n",
    "#train data \n",
    "train_data= df.iloc[X_train_inds]\n",
    "X_train = train_data.loc[:, ~train_data.columns.isin(['d_id','target_score'])]\n",
    "y_train = train_data.loc[:, train_data.columns.isin(['target_score'])]\n",
    "#test data \n",
    "test_data= df.iloc[X_test_inds]\n",
    "X_test = test_data.loc[:, ~test_data.columns.isin(['target_score','d_id'])]\n",
    "y_test = test_data.loc[:, test_data.columns.isin(['target_score','q_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d1d84",
   "metadata": {},
   "source": [
    "## Cross-Validation and Hyperparameter Tuning:\n",
    "1. Initially, we define an evaluation function leveraging NDCG scores, which aligns perfectly with the requirements of our ranking task. \n",
    "2. Subsequently, we employed a combination of grid search and cross-validation techniques exclusively on the training data to identify the optimal set of hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19044481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model using NDCG score.\n",
    "def eval_metric(model, X_val, y_val, groups_val):\n",
    "\n",
    "    # predictions from the model\n",
    "    preds = model.predict(X_val)\n",
    "    \n",
    "    # to handle groups \n",
    "    group_start_idx = 0\n",
    "    \n",
    "    ndcg_scores = []\n",
    "    \n",
    "    for group_size in groups_val:\n",
    "        end_idx = group_start_idx + group_size\n",
    "        \n",
    "        # true labels and predictions for the current group\n",
    "        true_labels = y_val.iloc[group_start_idx:end_idx].to_numpy().reshape(-1)\n",
    "        group_preds = preds[group_start_idx:end_idx]\n",
    "        ndcg_score_val = ndcg_score([true_labels], [group_preds])\n",
    "        ndcg_scores.append(ndcg_score_val)\n",
    "        \n",
    "        group_start_idx = end_idx  # move to the next group\n",
    "    \n",
    "    return np.mean(ndcg_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6162674f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 4/4 [33:29<00:00, 502.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'learning_rate': 0.001, 'n_estimators': 50, 'max_depth': 3}\n"
     ]
    }
   ],
   "source": [
    "# define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1,0.01,0.001],\n",
    "    'n_estimators': [50,100,150,200],\n",
    "    'max_depth': [3, 6, 9]\n",
    "    \n",
    "}\n",
    "\n",
    "# best parameters and score\n",
    "best_params = None\n",
    "best_score = float('inf')\n",
    "\n",
    "# cross-validation based on group ('q_id')\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "for n_estimators in tqdm(param_grid['n_estimators']):\n",
    "    for learning_rate in param_grid['learning_rate']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            scores = []\n",
    "            for train_idx, val_idx in gkf.split(X_train, y_train, groups=train_data['q_id']):\n",
    "                X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "                groups_train_fold = train_data.iloc[train_idx].groupby('q_id').size().to_numpy()\n",
    "                groups_val_fold = train_data.iloc[val_idx].groupby('q_id').size().to_numpy()\n",
    "\n",
    "                model = xgb.XGBRanker(\n",
    "                    objective='rank:pairwise',\n",
    "                    learning_rate=learning_rate,\n",
    "                    n_estimators=n_estimators,\n",
    "                    max_depth=max_depth,\n",
    "                )\n",
    "                \n",
    "                model.fit(X_train_fold, y_train_fold, group=groups_train_fold, verbose=False)\n",
    "                score = eval_metric(model, X_val_fold, y_val_fold, groups_val_fold) \n",
    "                scores.append(score)\n",
    "            \n",
    "            avg_score = np.mean(scores)\n",
    "            if avg_score < best_score:\n",
    "                best_score = avg_score\n",
    "                best_params = {\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'max_depth': max_depth,\n",
    "                }\n",
    "\n",
    "# train model with the best parameters\n",
    "model = xgb.XGBRanker(\n",
    "    objective='rank:pairwise',\n",
    "    **best_params\n",
    ")\n",
    "groups = train_data.groupby('q_id').size().to_numpy()\n",
    "model.fit(X_train, y_train, group=groups, verbose=True)\n",
    "\n",
    "print(\"Best parameters found: \", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe6379",
   "metadata": {},
   "source": [
    "## Testing Phase:\n",
    "In the final stage of our modeling process, we:\n",
    "1. Train the model on the training data using the best set of hyperparameters.\n",
    "2. Evaluate the model's performance on the test data by calculating the NDCG score. \n",
    "3. Save the trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a25c7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean NDCG Score across all groups in test data: 0.9260870670857707\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# predict the scores using the trained model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# calculate NDCG scores \n",
    "ndcg_scores = []\n",
    "\n",
    "#grouping by 'q_id' \n",
    "test_groups = test_data.groupby('q_id').size().to_numpy()\n",
    "\n",
    "start_idx = 0\n",
    "for group_size in test_groups:\n",
    "    end_idx = start_idx + group_size\n",
    "    true_relevance = [y_test[start_idx:end_idx]['target_score'].values]\n",
    "    scores_pred = np.asarray([y_pred[start_idx:end_idx]])\n",
    "    ndcg_score_val = ndcg_score(true_relevance, scores_pred)\n",
    "    ndcg_scores.append(ndcg_score_val)\n",
    "    start_idx = end_idx\n",
    "\n",
    "mean_ndcg_score = np.mean(ndcg_scores)\n",
    "\n",
    "print(\"Mean NDCG Score across all groups in test data:\", mean_ndcg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1edfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save_model('xgbranker_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d52845",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digikala",
   "language": "python",
   "name": "digikala"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
